{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy,en_core_web_sm\n",
    "from pathlib import Path\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "import json\n",
    "from benepar.spacy_plugin import BeneparComponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems: \n",
    "    1. from wsj_1001: sentence: \n",
    "    Rated Ba-3 by Moody's Investors Service Inc. and single-B-plus by Standard & Poor's Corp., the issue will be sold through Merrill Lynch Capital Markets. \n",
    "    tokenizer separate the sentence to: \"plus by Standard & Poor's Corp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(r'/home/pengfei/documents/stanford-corenlp-full-2018-10-05/')\n",
    "tokenizor = spacy.load('en')\n",
    "tokenizor.add_pipe(BeneparComponent(\"benepar_en2\"))\n",
    "DATA_PATH = Path(\"/home/pengfei/data/experiment/\")\n",
    "PDTB_PARSE = 'pdtb-parses.json'\n",
    "PDTB_DATA = 'pdtb-data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/miniconda3/envs/stanfordnlp/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "pdtb2 = pd.read_csv(\"../../pdtb2/pdtb2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linker(words, doc_lookup):\n",
    "    ## TODO: connective also has linker\n",
    "    \"\"\"\n",
    "    get look up for each word per doc level, if word does not below to any arg, it will append []\n",
    "    Args:\n",
    "            words(dict): {word_index_in_doc: [word_start_char, word_end_char]}\n",
    "            doc_lookup: from function @get_data_prototype\n",
    "    Returns: \n",
    "            ret: {word_index_in_doc: ['arg1_argID', 'arg2_argID', ...]}\n",
    "    \"\"\"\n",
    "    lookup_list = doc_lookup.keys()\n",
    "    ret = {}\n",
    "    \n",
    "    for w_idx, w_range in words.items():\n",
    "        w_linkers = []\n",
    "        for r in doc_lookup.keys():\n",
    "            r_list = _get_span_list(r)\n",
    "            if _in_between(w_range, r_list):\n",
    "                linker = doc_lookup[r]\n",
    "                w_linkers.append(linker)\n",
    "        ret[w_idx] = w_linkers\n",
    "    return ret\n",
    "\n",
    "def _in_between(inner_list, outer_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            inner_list(list[int]): [3,4]\n",
    "            outer_list(list[list[int]]): [[2,5]]\n",
    "    Returns: \n",
    "            boolean\n",
    "    \"\"\"\n",
    "    if len(outer_list) < 1:\n",
    "        return False\n",
    "    elif type(outer_list[0]) == str:\n",
    "        return (inner_list[0] >= int(outer_list[0])) & (inner_list[1] <= int(outer_list[1]))\n",
    "    elif len(outer_list) == 2:\n",
    "        for r in outer_list:\n",
    "            if _in_between(inner_list, r):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    return (inner_list[0] >= int(outer_list[0][0])) & (inner_list[1] <= int(outer_list[0][1]))\n",
    "        \n",
    "    \n",
    "def _get_span_list(span):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            span(str): \"34..96;97..101\"\n",
    "    Returns:\n",
    "            ret(list[list[str]]):\n",
    "            if one arg has one segment: [[34,96]]\n",
    "            if one arg has two segment: example [[34, 96], [97, 101]]\n",
    "    \"\"\"\n",
    "    if type(span) == float:\n",
    "        return []\n",
    "    spans = span.split(';')\n",
    "    return [o.split('..') for o in spans]\n",
    "        \n",
    "def get_batch(section, filenumber):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "            section(int)\n",
    "            filenumber(int)\n",
    "    Returns:\n",
    "            (list[int]): list of index with all the file with the same format in that batch\"\"\"\n",
    "    return pdtb2.index[(pdtb2['Section'] == section) & (pdtb2.FileNumber == filenumber)].tolist()\n",
    "# get_batch(0, 4)\n",
    "\n",
    "\n",
    "def get_data_prototype(section, filenumber, relation_id, batch_idx):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            section(int)\n",
    "            filenumber(int)\n",
    "            relation_id(int): relation_id is unique accross every relation, this is start relation_id\n",
    "            batch_idx(list[int]): list of corresponding int from same file\n",
    "    Returns:\n",
    "            doc_data: [{\"Arg1\": {\"CharacterSpanList\": [[4564, 4610]], \n",
    "                \"RawText\": \"\", \n",
    "                \"TokenList\": []}, # example: [4612, 4616, 888, 32, 11]\n",
    "                \"DocID\": \"wsj_1000\", \n",
    "                \"ID\": 15025, \n",
    "                \"Sense\": [\"Contingency.Condition\"], \n",
    "                \"Type\": \"Explicit\"},\n",
    "                {}, ...]\n",
    "            doc_lookup: dictionary contains all the span for argument 1 or argument 2 or conn\n",
    "                used for function @get_linker\n",
    "                {\"34..36;90..107\": [\"arg1_1234\"]}\n",
    "                \n",
    "            \n",
    "    \"\"\"\n",
    "    doc_data = []\n",
    "    doc_lookup = {}\n",
    "    for i, idx in enumerate(batch_idx):\n",
    "        arg1_span = pdtb2.loc[idx, 'Arg1_SpanList']\n",
    "        arg1_char_span_list = _get_span_list(arg1_span)\n",
    "        arg1_rawtext = pdtb2.loc[idx, 'Arg1_RawText']\n",
    "        arg1_token_list = []\n",
    "        \n",
    "        arg2_span = pdtb2.loc[idx, 'Arg2_SpanList']\n",
    "        arg2_char_span_list = _get_span_list(arg2_span)\n",
    "        arg2_rawtext = pdtb2.loc[idx, 'Arg2_RawText']\n",
    "        arg2_token_list = []\n",
    "        \n",
    "        conn_span = pdtb2.loc[idx, 'Connective_SpanList']\n",
    "        conn_char_span_list = _get_span_list(conn_span)\n",
    "        conn_rawtext = pdtb2.loc[idx, 'Connective_RawText']\n",
    "        conn_token_list = []\n",
    "        \n",
    "        relation_type = pdtb2.loc[idx, 'Relation']\n",
    "        relation_sense = [pdtb2.loc[idx, 'ConnHeadSemClass1']]\n",
    "        if relation_type == 'NoRel' or relation_type == 'EntRel':  \n",
    "            relation_sense = [relation_type]\n",
    "        elif type(pdtb2.loc[idx, 'ConnHeadSemClass2']) == str:\n",
    "            relation_sense.append(pdtb2.loc[idx, 'ConnHeadSemClass2'])\n",
    "        Doc_id = str(section) + '/' + str(filenumber)\n",
    "        \n",
    "        \n",
    "        ID = relation_id+i\n",
    "        \n",
    "        doc_lookup[arg1_span] = 'Arg1_' + str(ID)\n",
    "        doc_lookup[arg2_span] = 'Arg2_' + str(ID)\n",
    "        doc_lookup[conn_span] = 'Connective_' + str(ID)\n",
    "        \n",
    "        arg1 = {\"CharacterSpanList\": arg1_char_span_list, \"RawText\": arg1_rawtext, \"TokenList\": arg1_token_list}\n",
    "        arg2 = {\"CharacterSpanList\": arg2_char_span_list, \"RawText\": arg2_rawtext, \"TokenList\": arg2_token_list}\n",
    "        conn = {\"CharacterSpanList\": conn_span, \"RawText\": conn_rawtext, \"TokenList\": conn_token_list}\n",
    "        relation_dict = {\"Arg1\": arg1, \"Arg2\": arg2, \"Connective\": conn, \"DocID\": Doc_id, \"ID\": ID, \n",
    "                      \"Sense\": relation_sense, \"Type\": relation_type}\n",
    "        doc_data.append(relation_dict)\n",
    "        \n",
    "    return doc_data, doc_lookup\n",
    "\n",
    "def _get_files(data_path=DATA_PATH):\n",
    "    \"\"\"Returns (list): [('00', 'wsj_0000'), ..., ('24', 'wsj_2400']\"\"\"\n",
    "    sections = os.listdir(data_path)\n",
    "    ret = []\n",
    "    for sec in sections:\n",
    "        for filename in os.listdir(data_path/sec):\n",
    "            ret.append((sec, filename))\n",
    "    return ret\n",
    "\n",
    "def read_file(section, filename, data_path=DATA_PATH):\n",
    "    \"\"\"Returns the content in that file\"\"\"\n",
    "    with open(data_path/section/filename) as f:\n",
    "        file = f.read()\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constituent_parsing(sent):\n",
    "    return str(sent._.parse_string).replace('\\n', \" \").replace('   ', ' ').replace('  ', ' ')\n",
    "\n",
    "def dependency_parse(sent):\n",
    "    s = nlp.word_tokenize(sent)\n",
    "    tokens = [token for token in s]\n",
    "    tokens.insert(0,'ROOT')\n",
    "    dependency = []\n",
    "    for tag, start, end in nlp.dependency_parse(sent):\n",
    "        dependency.append([tag, f'{tokens[start]}-{start}', f'{tokens[end]}-{end}'])\n",
    "    return dependency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index(doc):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            doc: after tokenized document\n",
    "    Returns: \n",
    "            words(dict): {word_index_in_doc: [word_start_char, word_end_char]}\n",
    "    \"\"\"\n",
    "    words = {}\n",
    "    for tok in doc:\n",
    "        words[tok.i] = [tok.idx, tok.idx+len(tok)]\n",
    "    return words\n",
    "\n",
    "def add_token_list(doc_data, linkers, token_list):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "            doc_data: return value from @function get_data_prototype\n",
    "            linker(list[str]): [\"arg1_1234\", \"conn_1235\"]\n",
    "            token_list: append at token_list\n",
    "    \"\"\"\n",
    "    for linker in linkers:\n",
    "        arg_id = linker.split('_')[1]\n",
    "        \n",
    "        for rel in doc_data:\n",
    "            if rel[\"ID\"] == int(arg_id):\n",
    "                rel[linker.split('_')[0]][\"TokenList\"].append(token_list)\n",
    "\n",
    "def list_strip_punctuation(list):\n",
    "    \"\"\"Args:list[str]\"\"\"\n",
    "    punctuation = \"\"\"!\"#&'*+,-..../:;<=>?@[\\]^_`|~\"\"\" + \"``\" + \"''\"\n",
    "    i = 0\n",
    "    while i < len(list) and list[i][0] in punctuation + \"-LCB--LRB-\":\n",
    "        i += 1\n",
    "    if i == len(list):\n",
    "        return []\n",
    "    j = len(list) - 1\n",
    "    while j >= 0 and list[j][0] in punctuation + \"-RRB--RCB-\":\n",
    "        j -= 1\n",
    "    return list[i: j+1]\n",
    "\n",
    "\n",
    "def data_generator_per_doc_token_level(section, filenumber, relation_id, batch_idx, rawtext):\n",
    "    \"\"\"generate json format data for one document\n",
    "    Args:\n",
    "            section(str): 0~24\n",
    "            filenumber(str): 0~99\n",
    "            relation_id(int): relation_id is unique accross every relation, this is start relation_id\n",
    "            batch_idx(list[int]): list of corresponding int from same file\n",
    "            rawtext(str): rawtext of that doc, this rawtext is purged of \\n\n",
    "    Returns:\n",
    "            pdtb-json\n",
    "            pdtb-data\n",
    "    \"\"\"\n",
    "    # for data\n",
    "    doc_data, doc_lookup = get_data_prototype(section, filenumber, relation_id, batch_idx)\n",
    "    doc = tokenizor(rawtext)\n",
    "    words = get_word_index(doc)\n",
    "    linkers = get_linker(words, doc_lookup)\n",
    "    \n",
    "    # for parse\n",
    "    sentences = []\n",
    "    # for each token in document\n",
    "    for sentence_idx, sentence in enumerate(doc.sents):\n",
    "        ## TODO: here need to change\n",
    "        if sentence_idx == 0:\n",
    "            continue\n",
    "        else:   \n",
    "            # begin of the new round\n",
    "            words = []\n",
    "            sent = str(sentence)\n",
    "            dependencies = dependency_parse(sent)\n",
    "            parsetree = constituent_parsing(sentence)\n",
    "            \n",
    "            if section == 10 and filenumber == 1 and sentence_idx == 3:\n",
    "                print(parsetree)\n",
    "                print(sent)\n",
    "            for tok_idx_in_sentence, tok in enumerate(sentence):\n",
    "                ## 5 attribute of each word\n",
    "                char_offset_begin = tok.idx\n",
    "                char_offset_end = tok.idx + len(tok)\n",
    "                token_offset_in_doc = tok.i\n",
    "                sentence_offset = sentence_idx\n",
    "                token_offset_in_sent = tok_idx_in_sentence\n",
    "\n",
    "                linker = linkers[token_offset_in_doc]\n",
    "                token_dict = {'CharacterOffsetBegin': char_offset_begin, \"CharacterOffsetEnd\": char_offset_end, \n",
    "                              \"PartOfSpeech\": tok.pos_, \"Linkers\": linker}\n",
    "                words.append([tok.text, token_dict])\n",
    "\n",
    "                token_list = [char_offset_begin, char_offset_end, token_offset_in_doc, \n",
    "                              sentence_offset, token_offset_in_sent]\n",
    "                add_token_list(doc_data, linker, token_list)\n",
    "            \n",
    "            if list_strip_punctuation(words) == []:\n",
    "                continue\n",
    "            sentences.append({\"dependencies\": dependencies, \"parsetree\": parsetree, \"words\": words})\n",
    "    return {'sentences': sentences} , doc_data\n",
    "\n",
    "            \n",
    "def data_generator():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "            docs_parse(dict{docid: parse})\n",
    "            docs_data(list[relation])\n",
    "    \"\"\"\n",
    "    doc_paths = _get_files()\n",
    "    relation_id = 0\n",
    "    docs_parse = {}\n",
    "    docs_data = []\n",
    "    for doc_path in doc_paths:\n",
    "        section = doc_path[0]\n",
    "        filename = doc_path[1]\n",
    "        print(\"section: \", section, \" filename: \", filename)\n",
    "        rawtext = read_file(section, filename)\n",
    "        rawtext = rawtext.replace('\\n', '')\n",
    "        section = int(section)\n",
    "        filenumber = int(filename[-2:])\n",
    "        batch_idx = get_batch(section, filenumber)\n",
    "        \n",
    "        doc_parse, doc_data = data_generator_per_doc_token_level(section, filenumber, relation_id, batch_idx, rawtext)\n",
    "        relation_id += len(batch_idx)\n",
    "        \n",
    "        docs_parse[filename] = doc_parse\n",
    "        docs_data.extend(doc_data)\n",
    "        \n",
    "    return docs_parse, docs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parse(docs_parse, filename=PDTB_PARSE):\n",
    "    with open(filename, 'w') as f:\n",
    "        content = json.dumps(docs_parse)\n",
    "        f.write(content)\n",
    "\n",
    "def write_data(relations, filename=PDTB_DATA):\n",
    "    with open(filename, 'w') as f:\n",
    "        for relation in relations:\n",
    "            content = json.dumps(relation)\n",
    "            f.write(content)\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section:  01  filename:  wsj_0138\n",
      "section:  01  filename:  wsj_0139\n",
      "section:  00  filename:  wsj_0001\n",
      "section:  00  filename:  wsj_0002\n",
      "section:  10  filename:  wsj_1001\n",
      "(S (NP (DT 8%.The) (NNS notes)) (VP (VP (VBP are) (NP (JJ zero) (: -) (NN coupon) (NNS securities))) (CC and) (VP (MD will) (RB not) (VP (VB pay) (NP (NN interest)) (ADVP (RB periodically))))) (. .))\n",
      "8%.The notes are zero-coupon securities and will not pay interest periodically.\n",
      "section:  10  filename:  wsj_1000\n"
     ]
    }
   ],
   "source": [
    "docs_parse, docs_data = data_generator()\n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_parse['wsj_1001']['sentences'][3]['parsetree'] = \"(S (NP (NP (DT The) (NN size)) (PP (IN of) (NP (DT the) (NN offering)))) (VP (VBD was) (VP (VBN increased) (PP (IN from) (NP (NP (DT the) (ADJP (RB originally) (VBN planned)) (QP ($ $) (CD 250) (CD million))) (PRN (-LRB- -LRB-) (NP (NN redemption) (NN amount)) (-RRB- -RRB-)))))) (. .)) (S (NP (DT The) (NNS notes)) (VP (VBP are) (ADJP (JJ convertible) (PP (IN into) (NP (NP (JJ common) (NN stock)) (PP (IN of) (NP (NNP Blockbuster) (NNP Entertainment))))) (PP (IN at) (NP (NP (NP ($ $) (CD 22.26)) (NP (DT a) (NN share))) (, ,) (VP (VBG representing) (NP (NP (DT a) (ADJP (CD 12) (NN %)) (NN conversion) (NN premium)) (PP (IN over) (NP (NP (NN yesterday) (POS 's)) (NN closing) (NN price))))))))) (. .))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_parse(docs_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_data(docs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH/'01'/'wsj_0138') as f:\n",
    "    rawtext = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rawtext has error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (NP (NP (DT The) (NN size)) (PP (IN of) (NP (DT the) (NN offering)))) (VP (VBD was) (VP (VBN increased) (PP (IN from) (NP (NP (DT the) (ADJP (RB originally) (VBN planned)) (QP ($ $) (CD 250) (CD million))) (PRN (-LRB- -LRB-) (NP (NN redemption) (NN amount)) (-RRB- -RRB-)))))) (. .))\n",
      "(S (NP (DT The) (NNS notes)) (VP (VBP are) (ADJP (JJ convertible) (PP (IN into) (NP (NP (JJ common) (NN stock)) (PP (IN of) (NP (NNP Blockbuster) (NNP Entertainment))))) (PP (IN at) (NP (NP (NP ($ $) (CD 22.26)) (NP (DT a) (NN share))) (, ,) (VP (VBG representing) (NP (NP (DT a) (ADJP (CD 12) (NN %)) (NN conversion) (NN premium)) (PP (IN over) (NP (NP (NN yesterday) (POS 's)) (NN closing) (NN price))))))))) (. .))\n"
     ]
    }
   ],
   "source": [
    "sent = \"The size of the offering was increased from the originally planned $ 250 million ( redemption amount ) . The notes are convertible into common stock of Blockbuster Entertainment at $ 22.26 a share , representing a 12 % conversion premium over yesterday 's closing price .\"\n",
    "doc = tokenizor(sent)\n",
    "for sent in doc.sents:\n",
    "    print(constituent_parsing(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
