{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy,en_core_web_sm\n",
    "from pathlib import Path\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "tokenizor = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"/home/pengfei/data/pdtb_v2/data/raw/wsj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(916, 936)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# char off set start\n",
    "k.idx\n",
    "# token offset\n",
    "k.i\n",
    "# k.sent.start, k.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence offset in a doc\n",
    "list(doc.sents).index(k.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# character offset in a sentence\n",
    "list(k.sent).index(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/miniconda3/envs/stanfordnlp/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "pdtb2 = pd.read_csv(\"../pdtb2/pdtb2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relation</th>\n",
       "      <th>Section</th>\n",
       "      <th>FileNumber</th>\n",
       "      <th>Connective_SpanList</th>\n",
       "      <th>Connective_GornList</th>\n",
       "      <th>Connective_Trees</th>\n",
       "      <th>Connective_RawText</th>\n",
       "      <th>Connective_StringPosition</th>\n",
       "      <th>SentenceNumber</th>\n",
       "      <th>ConnHead</th>\n",
       "      <th>...</th>\n",
       "      <th>Arg2_Attribution_RawText</th>\n",
       "      <th>Sup1_SpanList</th>\n",
       "      <th>Sup1_GornList</th>\n",
       "      <th>Sup1_Trees</th>\n",
       "      <th>Sup1_RawText</th>\n",
       "      <th>Sup2_SpanList</th>\n",
       "      <th>Sup2_GornList</th>\n",
       "      <th>Sup2_Trees</th>\n",
       "      <th>Sup2_RawText</th>\n",
       "      <th>FullRawText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EntRel</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pierre Vinken, 61 years old, will join the boa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explicit</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>258..262</td>\n",
       "      <td>1,0,1,2,0</td>\n",
       "      <td>(IN once)</td>\n",
       "      <td>once</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>once</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The asbestos fiber, crocidolite, is unusually ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EntRel</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>379.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The asbestos fiber, crocidolite, is unusually ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explicit</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>534..542</td>\n",
       "      <td>3,0,0</td>\n",
       "      <td>(IN Although)</td>\n",
       "      <td>Although</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>although</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Although preliminary findings were reported mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Implicit</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>778.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is an old story. We're talking about year...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Relation  Section  FileNumber Connective_SpanList Connective_GornList  \\\n",
       "0    EntRel        0           1                 NaN                 NaN   \n",
       "1  Explicit        0           3            258..262           1,0,1,2,0   \n",
       "2    EntRel        0           3                 NaN                 NaN   \n",
       "3  Explicit        0           3            534..542               3,0,0   \n",
       "4  Implicit        0           3                 NaN                 NaN   \n",
       "\n",
       "  Connective_Trees Connective_RawText  Connective_StringPosition  \\\n",
       "0              NaN                NaN                       94.0   \n",
       "1       (IN once)                once                        NaN   \n",
       "2              NaN                NaN                      379.0   \n",
       "3   (IN Although)            Although                        NaN   \n",
       "4              NaN                NaN                      778.0   \n",
       "\n",
       "   SentenceNumber  ConnHead  ... Arg2_Attribution_RawText Sup1_SpanList  \\\n",
       "0             1.0       NaN  ...                      NaN           NaN   \n",
       "1             NaN      once  ...                      NaN           NaN   \n",
       "2             2.0       NaN  ...                      NaN           NaN   \n",
       "3             NaN  although  ...                      NaN           NaN   \n",
       "4             5.0       NaN  ...                      NaN           NaN   \n",
       "\n",
       "  Sup1_GornList Sup1_Trees Sup1_RawText Sup2_SpanList Sup2_GornList  \\\n",
       "0           NaN        NaN          NaN           NaN           NaN   \n",
       "1           NaN        NaN          NaN           NaN           NaN   \n",
       "2           NaN        NaN          NaN           NaN           NaN   \n",
       "3           NaN        NaN          NaN           NaN           NaN   \n",
       "4           NaN        NaN          NaN           NaN           NaN   \n",
       "\n",
       "  Sup2_Trees Sup2_RawText                                        FullRawText  \n",
       "0        NaN          NaN  Pierre Vinken, 61 years old, will join the boa...  \n",
       "1        NaN          NaN  The asbestos fiber, crocidolite, is unusually ...  \n",
       "2        NaN          NaN  The asbestos fiber, crocidolite, is unusually ...  \n",
       "3        NaN          NaN  Although preliminary findings were reported mo...  \n",
       "4        NaN          NaN  This is an old story. We're talking about year...  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdtb2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data_path, section, file_number):\n",
    "    filename = 'wsj_{0:04}'.format(int(file_number))\n",
    "    section = '{0:02}'.format(int(section))\n",
    "    with open(data_path/section/filename) as f:\n",
    "        file = f.read()\n",
    "    return file\n",
    "\n",
    "file_content = read_file(DATA_PATH, '0', '96')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(100):\n",
    "    section = pdtb2.loc[i, 'Section']\n",
    "    filenumber = pdtb2.loc[i, 'FileNumber']\n",
    "    file_content = read_file(DATA_PATH, section, filenumber)\n",
    "    arg1_spanlist = pdtb2.loc[i, 'Arg1_SpanList']\n",
    "    arg2_spanlist = pdtb2.loc[i, 'Arg2_SpanList']\n",
    "    arg1_spanlist = arg1_spanlist = arg1_spanlist.split(';')\n",
    "    arg1 = []\n",
    "    for span in arg1_spanlist: \n",
    "        spanlist = span.split('..')\n",
    "        arg1.append(file_content[int(spanlist[0]): int(spanlist[1])])\n",
    "    arg2_spanlist = arg2_spanlist = arg2_spanlist.split(';')\n",
    "    arg2 = []\n",
    "    for span in arg2_spanlist: \n",
    "        spanlist = span.split('..')\n",
    "        arg2.append(file_content[int(spanlist[0]): int(spanlist[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 27\n",
    "section = pdtb2.loc[i, 'Section']\n",
    "filenumber = pdtb2.loc[i, 'FileNumber']\n",
    "file_content = read_file(DATA_PATH, section, filenumber)\n",
    "arg1_spanlist = pdtb2.loc[i, 'Arg1_SpanList']\n",
    "arg2_spanlist = pdtb2.loc[i, 'Arg2_SpanList']\n",
    "arg1_spanlist = arg1_spanlist = arg1_spanlist.split(';')\n",
    "arg1 = []\n",
    "for span in arg1_spanlist: \n",
    "    spanlist = span.split('..')\n",
    "    arg1.append(file_content[int(spanlist[0]): int(spanlist[1])])\n",
    "arg2_spanlist = arg2_spanlist = arg2_spanlist.split(';')\n",
    "arg2 = []\n",
    "for span in arg2_spanlist: \n",
    "    spanlist = span.split('..')\n",
    "    arg2.append(file_content[int(spanlist[0]): int(spanlist[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ['Nevertheless', 'yields \"may blip up again']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(section, filenumber):\n",
    "    file_content = read_file(DATA_PATH, section, filenumber)\n",
    "    doc = tokenizor(file_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = tokenizor(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connective_SpanList\n",
      "Connective_GornList\n",
      "Connective_Trees\n",
      "Connective_RawText\n",
      "Connective_StringPosition\n",
      "ConnHead\n",
      "Conn1\n",
      "Conn2\n",
      "ConnHeadSemClass1\n",
      "ConnHeadSemClass2\n",
      "Conn2SemClass1\n",
      "Conn2SemClass2\n"
     ]
    }
   ],
   "source": [
    "for i in pdtb2.keys():\n",
    "    if 'conn' in i or 'Conn' in i or 'Sense' in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit in fact\n",
      "Implicit besides\n",
      "Implicit accordingly\n",
      "Implicit in addition\n",
      "Implicit and\n",
      "Implicit as a result\n",
      "Implicit specifically\n",
      "Implicit because\n",
      "Implicit in other words\n",
      "Implicit on the other hand\n",
      "Implicit for example\n",
      "Implicit for instance\n",
      "Implicit for example\n",
      "Implicit and\n",
      "Implicit and\n",
      "Implicit then\n",
      "Implicit then\n",
      "Implicit consequently\n",
      "Implicit but\n",
      "Implicit as a result\n",
      "Implicit for example\n",
      "Implicit and\n",
      "Implicit in fact\n",
      "Implicit for example\n",
      "Implicit by comparison\n",
      "Implicit thus\n",
      "Implicit meanwhile\n",
      "Implicit specifically\n",
      "Implicit specifically\n",
      "Implicit and\n",
      "Implicit and\n",
      "Implicit in other words\n",
      "Implicit in the end\n",
      "Implicit because\n",
      "Implicit but\n",
      "Implicit because\n",
      "Implicit but\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    if pdtb2.loc[i, 'Relation'] == 'Explicit':\n",
    "        print(type(pdtb2.loc[i, 'Connective_SpanList']) == str)\n",
    "    if pdtb2.loc[i, 'Relation'] == 'Implicit':\n",
    "        print(pdtb2.loc[i, 'Relation'], pdtb2.loc[i, 'Conn1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ConnHeadSemClass1'"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_linker(words, doc_lookup):\n",
    "    \"\"\"get look up for each word per doc level\n",
    "    Args:\n",
    "            word(dict): {word_index_in_doc: [word_start_char, word_end_char]}\n",
    "            doc_lookup: from function @get_data_prototype\n",
    "    Returns: \n",
    "            ret: {word_index_in_doc: 'arg1_argID'} \n",
    "    \"\"\"\n",
    "    lookup_list = doc_lookup.keys()\n",
    "    ret = {}\n",
    "    \n",
    "    for w_idx, w_range in words.iteritems():\n",
    "        w_linkers = []\n",
    "        for r in doc_lookup.keys():\n",
    "            if _in_between_(w_range, r):\n",
    "                linker = doc_lookup[r][0]\n",
    "                w_linkers.append(linker)\n",
    "        ret[w_idx] = w_linkers\n",
    "    return ret\n",
    "\n",
    "def _in_between(inner_list, outer_list):\n",
    "    return (inner_list[0] >= outer_list[0]) & (inner_list[1] <= outer_list[1])\n",
    "    \n",
    "def get_batch(section, filenumber):\n",
    "    \"\"\"Returns list of index with all the file with the same format in that batch\"\"\"\n",
    "    return pdtb2.index(pdtb2['Section'] == section & pdtb2['filenumber'] == filenumber).tolist()\n",
    "        \n",
    "    \n",
    "def _get_span_list(span):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            span(str): \"34..96;97..101\"\n",
    "    Returns:\n",
    "            ret(list[list]):\n",
    "            if one arg has one segment: [[34,96]]\n",
    "            if one arg has two segment: example [[34, 96], [97, 101]]\n",
    "    \"\"\"\n",
    "    spans = span.split(';')\n",
    "    return [o.split('..') for o in spans]\n",
    "        \n",
    "\n",
    "def get_data_prototype(section, filenumber, relation_id):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            relation_id(int): relation_id is unique accross every relation\n",
    "    Returns:\n",
    "            doc_data: [{\"Arg1\": {\"CharacterSpanList\": [[4564, 4610]], \n",
    "                \"RawText\": \"\", \n",
    "                \"TokenList\": []}, # example: [4612, 4616, 888, 32, 11]\n",
    "                \"DocID\": \"wsj_1000\", \n",
    "                \"ID\": 15025, \n",
    "                \"Sense\": [\"Contingency.Condition\"], \n",
    "                \"Type\": \"Explicit\"},\n",
    "                {}, ...]\n",
    "            doc_lookup: dictionary contains all the span for argument 1 or argument 2 or conn\n",
    "                used for function @get_linker\n",
    "                {[34, 45]: [\"arg1_1234\"]}\n",
    "                \n",
    "            \n",
    "    \"\"\"\n",
    "    batch_idx = get_batch(section, filenumber)\n",
    "    doc_data = []\n",
    "    doc_lookup = {}\n",
    "    for idx in batch_idx:\n",
    "        arg1_span = pdtb2.loc[idx, 'Arg1_SpanList']\n",
    "        arg1_char_span_list = _get_span_list(arg1_span)\n",
    "        arg1_rawtext = pdtb2.loc[idx, 'Arg1_RawText']\n",
    "        arg1_token_list = []\n",
    "        \n",
    "        arg2_span = pdtb2.loc[idx, 'Arg2_SpanList']\n",
    "        arg2_char_span_list = _get_span_list(arg2_span)\n",
    "        arg2_rawtext = pdtb2.loc[idx, 'Arg2_RawText']\n",
    "        arg2_token_list = []\n",
    "        \n",
    "        conn_span = pdtb2.loc[idx, 'Connective_SpanList']\n",
    "        conn_char_span_list = _get_span_list(conn_span)\n",
    "        conn_rawtext = pdtb2.loc[idx, 'Connective_RawText']\n",
    "        conn_token_list = []\n",
    "        \n",
    "        relation_type = pdtb2.loc[idx, 'Relation']\n",
    "        relation_sense = [pdtb2.loc[idx, 'ConnHeadSemClass1']]\n",
    "        if type(pdtb2.loc[idx, 'ConnHeadSemClass2']) == str:\n",
    "            relation_sense.append(pdtb2.loc[idx, 'ConnHeadSemClass2'])\n",
    "        Doc_id = section + '/' + filenumber\n",
    "        ID = relation_id+idx\n",
    "        \n",
    "        doc_lookup[arg1_char_span_list] = 'arg1_' + ID\n",
    "        doc_lookup[arg2_char_span_list] = 'arg2_' + ID\n",
    "        doc_lookup[conn_char_span_list] = 'conn_' + ID\n",
    "        \n",
    "        arg1 = {\"CharacterSpanList\": arg1_char_span_list, \"RawText\": arg1_rawtext, \"TokenList\": arg1_token_list}\n",
    "        arg2 = {\"CharacterSpanList\": arg2_char_span_list, \"RawText\": arg2_rawtext, \"TokenList\": arg2_token_list}\n",
    "        conn = {\"CharacterSpanList\": conn_span, \"RawText\" = conn_rawtext, \"TokenList\": conn_token_list}\n",
    "        relation_dict = {\"Arg1\": arg1, \"Arg2\": arg2, \"DocID\": Doc_id, \"ID\": ID, \n",
    "                      \"Sense\": relation_sense, \"Type\": relation_type}\n",
    "        doc_data.append(relation_dict)\n",
    "        \n",
    "    return doc_data, doc_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section(data_path):\n",
    "    \"\"\"return {section_num: [filenum1, filenum2...]}\"\"\"\n",
    "    sections = os.listdir(data_path)\n",
    "    ret = {}\n",
    "    for sec in sections:\n",
    "        ret[sec] = os.listdir(data_path/sec)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
