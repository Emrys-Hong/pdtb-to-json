{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy,en_core_web_sm\n",
    "from pathlib import Path\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(r'/home/pengfei/documents/stanford-corenlp-full-2018-10-05/')\n",
    "tokenizor = en_core_web_sm.load()\n",
    "DATA_PATH = Path(\"/home/pengfei/data/experiment/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/miniconda3/envs/stanfordnlp/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "pdtb2 = pd.read_csv(\"../pdtb2/pdtb2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linker(words, doc_lookup):\n",
    "    ## TODO: connective also has linker\n",
    "    \"\"\"\n",
    "    get look up for each word per doc level, if word does not below to any arg, it will append []\n",
    "    Args:\n",
    "            words(dict): {word_index_in_doc: [word_start_char, word_end_char]}\n",
    "            doc_lookup: from function @get_data_prototype\n",
    "    Returns: \n",
    "            ret: {word_index_in_doc: ['arg1_argID', 'arg2_argID', ...]}\n",
    "    \"\"\"\n",
    "    lookup_list = doc_lookup.keys()\n",
    "    ret = {}\n",
    "    \n",
    "    for w_idx, w_range in words.items():\n",
    "        w_linkers = []\n",
    "        for r in doc_lookup.keys():\n",
    "            r_list = _get_span_list(r)\n",
    "            if _in_between(w_range, r_list):\n",
    "                linker = doc_lookup[r]\n",
    "                w_linkers.append(linker)\n",
    "        ret[w_idx] = w_linkers\n",
    "    return ret\n",
    "\n",
    "def _in_between(inner_list, outer_list):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            inner_list(list[int]): [3,4]\n",
    "            outer_list(list[list[int]]): [[2,5]]\n",
    "    Returns: \n",
    "            boolean\n",
    "    \"\"\"\n",
    "    if len(outer_list) < 1:\n",
    "        return False\n",
    "    elif type(outer_list[0]) == str:\n",
    "        return (inner_list[0] >= int(outer_list[0])) & (inner_list[1] <= int(outer_list[1]))\n",
    "    elif len(outer_list) == 2:\n",
    "        for r in outer_list:\n",
    "            if _in_between(inner_list, r):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    return (inner_list[0] >= int(outer_list[0][0])) & (inner_list[1] <= int(outer_list[0][1]))\n",
    "        \n",
    "    \n",
    "def _get_span_list(span):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            span(str): \"34..96;97..101\"\n",
    "    Returns:\n",
    "            ret(list[list[str]]):\n",
    "            if one arg has one segment: [[34,96]]\n",
    "            if one arg has two segment: example [[34, 96], [97, 101]]\n",
    "    \"\"\"\n",
    "    if type(span) == float:\n",
    "        return []\n",
    "    spans = span.split(';')\n",
    "    return [o.split('..') for o in spans]\n",
    "        \n",
    "def get_batch(section, filenumber):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "            section(int)\n",
    "            filenumber(int)\n",
    "    Returns:\n",
    "            (list[int]): list of index with all the file with the same format in that batch\"\"\"\n",
    "    return pdtb2.index[(pdtb2['Section'] == section) & (pdtb2.FileNumber == filenumber)].tolist()\n",
    "# get_batch(0, 4)\n",
    "\n",
    "\n",
    "def get_data_prototype(section, filenumber, relation_id, batch_idx):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            section(int)\n",
    "            filenumber(int)\n",
    "            relation_id(int): relation_id is unique accross every relation, this is start relation_id\n",
    "            batch_idx(list[int]): list of corresponding int from same file\n",
    "    Returns:\n",
    "            doc_data: [{\"Arg1\": {\"CharacterSpanList\": [[4564, 4610]], \n",
    "                \"RawText\": \"\", \n",
    "                \"TokenList\": []}, # example: [4612, 4616, 888, 32, 11]\n",
    "                \"DocID\": \"wsj_1000\", \n",
    "                \"ID\": 15025, \n",
    "                \"Sense\": [\"Contingency.Condition\"], \n",
    "                \"Type\": \"Explicit\"},\n",
    "                {}, ...]\n",
    "            doc_lookup: dictionary contains all the span for argument 1 or argument 2 or conn\n",
    "                used for function @get_linker\n",
    "                {\"34..36;90..107\": [\"arg1_1234\"]}\n",
    "                \n",
    "            \n",
    "    \"\"\"\n",
    "    doc_data = []\n",
    "    doc_lookup = {}\n",
    "    for i, idx in enumerate(batch_idx):\n",
    "        arg1_span = pdtb2.loc[idx, 'Arg1_SpanList']\n",
    "        arg1_char_span_list = _get_span_list(arg1_span)\n",
    "        arg1_rawtext = pdtb2.loc[idx, 'Arg1_RawText']\n",
    "        arg1_token_list = []\n",
    "        \n",
    "        arg2_span = pdtb2.loc[idx, 'Arg2_SpanList']\n",
    "        arg2_char_span_list = _get_span_list(arg2_span)\n",
    "        arg2_rawtext = pdtb2.loc[idx, 'Arg2_RawText']\n",
    "        arg2_token_list = []\n",
    "        \n",
    "        conn_span = pdtb2.loc[idx, 'Connective_SpanList']\n",
    "        conn_char_span_list = _get_span_list(conn_span)\n",
    "        conn_rawtext = pdtb2.loc[idx, 'Connective_RawText']\n",
    "        conn_token_list = []\n",
    "        \n",
    "        relation_type = pdtb2.loc[idx, 'Relation']\n",
    "        relation_sense = [pdtb2.loc[idx, 'ConnHeadSemClass1']]\n",
    "        if relation_type == 'NoRel' or relation_type == 'EntRel':  \n",
    "            relation_sense = [relation_type]\n",
    "        elif type(pdtb2.loc[idx, 'ConnHeadSemClass2']) == str:\n",
    "            relation_sense.append(pdtb2.loc[idx, 'ConnHeadSemClass2'])\n",
    "        Doc_id = str(section) + '/' + str(filenumber)\n",
    "        \n",
    "        \n",
    "        ID = relation_id+i\n",
    "        \n",
    "        doc_lookup[arg1_span] = 'Arg1_' + str(ID)\n",
    "        doc_lookup[arg2_span] = 'Arg2_' + str(ID)\n",
    "        doc_lookup[conn_span] = 'Connective_' + str(ID)\n",
    "        \n",
    "        arg1 = {\"CharacterSpanList\": arg1_char_span_list, \"RawText\": arg1_rawtext, \"TokenList\": arg1_token_list}\n",
    "        arg2 = {\"CharacterSpanList\": arg2_char_span_list, \"RawText\": arg2_rawtext, \"TokenList\": arg2_token_list}\n",
    "        conn = {\"CharacterSpanList\": conn_span, \"RawText\": conn_rawtext, \"TokenList\": conn_token_list}\n",
    "        relation_dict = {\"Arg1\": arg1, \"Arg2\": arg2, \"Connective\": conn, \"DocID\": Doc_id, \"ID\": ID, \n",
    "                      \"Sense\": relation_sense, \"Type\": relation_type}\n",
    "        doc_data.append(relation_dict)\n",
    "        \n",
    "    return doc_data, doc_lookup\n",
    "\n",
    "def _get_files(data_path=DATA_PATH):\n",
    "    \"\"\"Returns (list): [('00', 'wsj_0000'), ..., ('24', 'wsj_2400']\"\"\"\n",
    "    sections = os.listdir(data_path)\n",
    "    ret = []\n",
    "    for sec in sections:\n",
    "        for filename in os.listdir(data_path/sec):\n",
    "            ret.append((sec, filename))\n",
    "    return ret\n",
    "\n",
    "def read_file(section, filename, data_path=DATA_PATH):\n",
    "    \"\"\"Returns the content in that file\"\"\"\n",
    "    with open(data_path/section/filename) as f:\n",
    "        file = f.read()\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constituent_parsing(sent):\n",
    "    return nlp.parse(sent).replace('ROOT', \" \").replace('\\n', '').replace(' ', '')\n",
    "\n",
    "def dependency_parse(sent):\n",
    "    s = tokenizor(sent)\n",
    "    tokens = [token.text for token in s]\n",
    "    tokens.insert(0,'ROOT')\n",
    "    dependency = []\n",
    "    for tag, start, end in nlp.dependency_parse(sent):\n",
    "        dependency.append([tag, f'{tokens[start]}-{start}', f'{tokens[end]}-{end}'])\n",
    "    return dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index(doc):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            doc: after tokenized document\n",
    "    Returns: \n",
    "            words(dict): {word_index_in_doc: [word_start_char, word_end_char]}\n",
    "    \"\"\"\n",
    "    words = {}\n",
    "    for tok in doc:\n",
    "        words[tok.i] = [tok.idx, tok.idx+len(tok)]\n",
    "    return words\n",
    "\n",
    "def add_token_list(doc_data, linkers, token_list):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "            doc_data: return value from @function get_data_prototype\n",
    "            linker(list[str]): [\"arg1_1234\", \"conn_1235\"]\n",
    "            token_list: append at token_list\n",
    "    \"\"\"\n",
    "    for linker in linkers:\n",
    "        arg_id = linker.split('_')[1]\n",
    "        \n",
    "        for rel in doc_data:\n",
    "            if rel[\"ID\"] == int(arg_id):\n",
    "                rel[linker.split('_')[0]][\"TokenList\"].append(token_list)\n",
    "\n",
    "def data_generator_per_doc(section, filenumber, relation_id, batch_idx, rawtext):\n",
    "    \"\"\"generate json format data for one document\n",
    "    Args:\n",
    "            section(str): 0~24\n",
    "            filenumber(str): 0~99\n",
    "            relation_id(int): relation_id is unique accross every relation, this is start relation_id\n",
    "            batch_idx(list[int]): list of corresponding int from same file\n",
    "            rawtext(str): rawtext of that doc\n",
    "    Returns:\n",
    "            pdtb-json\n",
    "            pdtb-data\n",
    "    \"\"\"\n",
    "    # for data\n",
    "    doc_data, doc_lookup = get_data_prototype(section, filenumber, relation_id, batch_idx)\n",
    "    doc = tokenizor(rawtext)\n",
    "    words = get_word_index(doc)\n",
    "    linkers = get_linker(words, doc_lookup)\n",
    "    \n",
    "    # for parse\n",
    "    temp = None\n",
    "    first_sent = None\n",
    "    sentences = []\n",
    "    # for each token in document\n",
    "    for tok in doc:\n",
    "        ## TODO: here need to change\n",
    "        if temp == None:\n",
    "            first_sent = str(tok.sent)\n",
    "            temp = True\n",
    "            continue\n",
    "        elif str(tok.sent) == first_sent:\n",
    "            continue\n",
    "        else:\n",
    "            if str(tok.sent) != temp:\n",
    "                if type(first_sent) != str:# end of the previous round\n",
    "                    sentences.append({\"dependencies\": dependencies, \"parsetree\": parsetree, \n",
    "                                      \"words\": words})\n",
    "                \n",
    "                # begin of the new round\n",
    "                words = []\n",
    "                temp = str(tok.sent)\n",
    "                first_sent = False\n",
    "                dependencies = dependency_parse(temp)\n",
    "                parsetree = constituent_parsing(temp)\n",
    "            \n",
    "            ## 5 attribute of each word\n",
    "            char_offset_begin = tok.idx\n",
    "            char_offset_end = tok.idx + len(tok)\n",
    "            token_offset_in_doc = tok.i\n",
    "            sentence_offset = list(doc.sents).index(tok.sent)\n",
    "            token_offset_in_sent = list(tok.sent).index(tok)\n",
    "            \n",
    "            linker = linkers[token_offset_in_doc]\n",
    "            token_dict = {'CharacterOffsetBegin': char_offset_begin, \"CharacterOffsetEnd\": char_offset_end, \n",
    "                          \"PartOfSpeech\": tok.pos_, \"Linkers\": linker}\n",
    "            words.append(token_dict)\n",
    "            \n",
    "            token_list = [char_offset_begin, char_offset_end, token_offset_in_doc, \n",
    "                          sentence_offset, token_offset_in_sent]\n",
    "            add_token_list(doc_data, linker, token_list)\n",
    "            \n",
    "    return {'sentences': sentences} , doc_data\n",
    "            \n",
    "def data_generator():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "            docs_parse(dict{docid: parse})\n",
    "            docs_data(list[relation])\n",
    "    \"\"\"\n",
    "    doc_paths = _get_files()\n",
    "    relation_id = 0\n",
    "    docs_parse = {}\n",
    "    docs_data = []\n",
    "    for doc_path in doc_paths:\n",
    "        section = doc_path[0]\n",
    "        filename = doc_path[1]\n",
    "        print(\"section: \", section, \" filename: \", filename)\n",
    "        rawtext = read_file(section, filename)\n",
    "        \n",
    "        section = int(section)\n",
    "        filenumber = int(filename[-2:])\n",
    "        batch_idx = get_batch(section, filenumber)\n",
    "        \n",
    "        doc_parse, doc_data = data_generator_per_doc(section, filenumber, relation_id, batch_idx, rawtext)\n",
    "        relation_id += len(batch_idx)\n",
    "        \n",
    "        docs_parse[filename] = doc_parse\n",
    "        docs_data.extend(doc_data)\n",
    "        \n",
    "    return docs_parse, docs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section:  01  filename:  wsj_0134\n",
      "section:  01  filename:  wsj_0136\n",
      "section:  01  filename:  wsj_0138\n",
      "section:  01  filename:  wsj_0139\n",
      "section:  01  filename:  wsj_0130\n",
      "section:  01  filename:  wsj_0135\n",
      "section:  01  filename:  wsj_0137\n",
      "section:  01  filename:  wsj_0131\n",
      "section:  01  filename:  wsj_0133\n",
      "section:  01  filename:  wsj_0132\n",
      "section:  00  filename:  wsj_0004\n",
      "section:  00  filename:  wsj_0008\n",
      "section:  00  filename:  wsj_0001\n",
      "section:  00  filename:  wsj_0006\n",
      "section:  00  filename:  wsj_0009\n",
      "section:  00  filename:  wsj_0002\n",
      "section:  00  filename:  wsj_0003\n",
      "section:  00  filename:  wsj_0007\n",
      "section:  00  filename:  wsj_0005\n"
     ]
    }
   ],
   "source": [
    "docs_parse, docs_data = data_generator()\n",
    "nlp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH/'00'/'wsj_0004') as f:\n",
    "    rawtext = f.read()\n",
    "batch = get_batch(0, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
