{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy,en_core_web_sm\n",
    "from pathlib import Path\n",
    "import math\n",
    "import pandas as pd\n",
    "import os\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "import json\n",
    "nlp = StanfordCoreNLP(r'/home/pengfei/documents/stanford-corenlp-full-2018-10-05/')\n",
    "tokenizor = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"/home/pengfei/data/pdtb_v2/data/raw/wsj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(916, 936)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# char off set start\n",
    "k.idx\n",
    "# token offset\n",
    "k.i\n",
    "# k.sent.start, k.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence offset in a doc\n",
    "list(doc.sents).index(k.sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# character offset in a sentence\n",
    "list(k.sent).index(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pengfei/miniconda3/envs/stanfordnlp/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "pdtb2 = pd.read_csv(\"../pdtb2/pdtb2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Relation</th>\n",
       "      <th>Section</th>\n",
       "      <th>FileNumber</th>\n",
       "      <th>Connective_SpanList</th>\n",
       "      <th>Connective_GornList</th>\n",
       "      <th>Connective_Trees</th>\n",
       "      <th>Connective_RawText</th>\n",
       "      <th>Connective_StringPosition</th>\n",
       "      <th>SentenceNumber</th>\n",
       "      <th>ConnHead</th>\n",
       "      <th>...</th>\n",
       "      <th>Arg2_Attribution_RawText</th>\n",
       "      <th>Sup1_SpanList</th>\n",
       "      <th>Sup1_GornList</th>\n",
       "      <th>Sup1_Trees</th>\n",
       "      <th>Sup1_RawText</th>\n",
       "      <th>Sup2_SpanList</th>\n",
       "      <th>Sup2_GornList</th>\n",
       "      <th>Sup2_Trees</th>\n",
       "      <th>Sup2_RawText</th>\n",
       "      <th>FullRawText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EntRel</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pierre Vinken, 61 years old, will join the boa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Explicit</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>258..262</td>\n",
       "      <td>1,0,1,2,0</td>\n",
       "      <td>(IN once)</td>\n",
       "      <td>once</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>once</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The asbestos fiber, crocidolite, is unusually ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EntRel</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>379.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The asbestos fiber, crocidolite, is unusually ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Explicit</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>534..542</td>\n",
       "      <td>3,0,0</td>\n",
       "      <td>(IN Although)</td>\n",
       "      <td>Although</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>although</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Although preliminary findings were reported mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Implicit</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>778.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is an old story. We're talking about year...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Relation  Section  FileNumber Connective_SpanList Connective_GornList  \\\n",
       "0    EntRel        0           1                 NaN                 NaN   \n",
       "1  Explicit        0           3            258..262           1,0,1,2,0   \n",
       "2    EntRel        0           3                 NaN                 NaN   \n",
       "3  Explicit        0           3            534..542               3,0,0   \n",
       "4  Implicit        0           3                 NaN                 NaN   \n",
       "\n",
       "  Connective_Trees Connective_RawText  Connective_StringPosition  \\\n",
       "0              NaN                NaN                       94.0   \n",
       "1       (IN once)                once                        NaN   \n",
       "2              NaN                NaN                      379.0   \n",
       "3   (IN Although)            Although                        NaN   \n",
       "4              NaN                NaN                      778.0   \n",
       "\n",
       "   SentenceNumber  ConnHead  ... Arg2_Attribution_RawText Sup1_SpanList  \\\n",
       "0             1.0       NaN  ...                      NaN           NaN   \n",
       "1             NaN      once  ...                      NaN           NaN   \n",
       "2             2.0       NaN  ...                      NaN           NaN   \n",
       "3             NaN  although  ...                      NaN           NaN   \n",
       "4             5.0       NaN  ...                      NaN           NaN   \n",
       "\n",
       "  Sup1_GornList Sup1_Trees Sup1_RawText Sup2_SpanList Sup2_GornList  \\\n",
       "0           NaN        NaN          NaN           NaN           NaN   \n",
       "1           NaN        NaN          NaN           NaN           NaN   \n",
       "2           NaN        NaN          NaN           NaN           NaN   \n",
       "3           NaN        NaN          NaN           NaN           NaN   \n",
       "4           NaN        NaN          NaN           NaN           NaN   \n",
       "\n",
       "  Sup2_Trees Sup2_RawText                                        FullRawText  \n",
       "0        NaN          NaN  Pierre Vinken, 61 years old, will join the boa...  \n",
       "1        NaN          NaN  The asbestos fiber, crocidolite, is unusually ...  \n",
       "2        NaN          NaN  The asbestos fiber, crocidolite, is unusually ...  \n",
       "3        NaN          NaN  Although preliminary findings were reported mo...  \n",
       "4        NaN          NaN  This is an old story. We're talking about year...  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdtb2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(100):\n",
    "    section = pdtb2.loc[i, 'Section']\n",
    "    filenumber = pdtb2.loc[i, 'FileNumber']\n",
    "    file_content = read_file(DATA_PATH, section, filenumber)\n",
    "    arg1_spanlist = pdtb2.loc[i, 'Arg1_SpanList']\n",
    "    arg2_spanlist = pdtb2.loc[i, 'Arg2_SpanList']\n",
    "    arg1_spanlist = arg1_spanlist = arg1_spanlist.split(';')\n",
    "    arg1 = []\n",
    "    for span in arg1_spanlist: \n",
    "        spanlist = span.split('..')\n",
    "        arg1.append(file_content[int(spanlist[0]): int(spanlist[1])])\n",
    "    arg2_spanlist = arg2_spanlist = arg2_spanlist.split(';')\n",
    "    arg2 = []\n",
    "    for span in arg2_spanlist: \n",
    "        spanlist = span.split('..')\n",
    "        arg2.append(file_content[int(spanlist[0]): int(spanlist[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 27\n",
    "section = pdtb2.loc[i, 'Section']\n",
    "filenumber = pdtb2.loc[i, 'FileNumber']\n",
    "file_content = read_file(DATA_PATH, section, filenumber)\n",
    "arg1_spanlist = pdtb2.loc[i, 'Arg1_SpanList']\n",
    "arg2_spanlist = pdtb2.loc[i, 'Arg2_SpanList']\n",
    "arg1_spanlist = arg1_spanlist = arg1_spanlist.split(';')\n",
    "arg1 = []\n",
    "for span in arg1_spanlist: \n",
    "    spanlist = span.split('..')\n",
    "    arg1.append(file_content[int(spanlist[0]): int(spanlist[1])])\n",
    "arg2_spanlist = arg2_spanlist = arg2_spanlist.split(';')\n",
    "arg2 = []\n",
    "for span in arg2_spanlist: \n",
    "    spanlist = span.split('..')\n",
    "    arg2.append(file_content[int(spanlist[0]): int(spanlist[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(section, filenumber):\n",
    "    file_content = read_file(DATA_PATH, section, filenumber)\n",
    "    doc = tokenizor(file_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = tokenizor(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connective_SpanList\n",
      "Connective_GornList\n",
      "Connective_Trees\n",
      "Connective_RawText\n",
      "Connective_StringPosition\n",
      "ConnHead\n",
      "Conn1\n",
      "Conn2\n",
      "ConnHeadSemClass1\n",
      "ConnHeadSemClass2\n",
      "Conn2SemClass1\n",
      "Conn2SemClass2\n"
     ]
    }
   ],
   "source": [
    "for i in pdtb2.keys():\n",
    "    if 'conn' in i or 'Conn' in i or 'Sense' in i:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implicit in fact\n",
      "Implicit besides\n",
      "Implicit accordingly\n",
      "Implicit in addition\n",
      "Implicit and\n",
      "Implicit as a result\n",
      "Implicit specifically\n",
      "Implicit because\n",
      "Implicit in other words\n",
      "Implicit on the other hand\n",
      "Implicit for example\n",
      "Implicit for instance\n",
      "Implicit for example\n",
      "Implicit and\n",
      "Implicit and\n",
      "Implicit then\n",
      "Implicit then\n",
      "Implicit consequently\n",
      "Implicit but\n",
      "Implicit as a result\n",
      "Implicit for example\n",
      "Implicit and\n",
      "Implicit in fact\n",
      "Implicit for example\n",
      "Implicit by comparison\n",
      "Implicit thus\n",
      "Implicit meanwhile\n",
      "Implicit specifically\n",
      "Implicit specifically\n",
      "Implicit and\n",
      "Implicit and\n",
      "Implicit in other words\n",
      "Implicit in the end\n",
      "Implicit because\n",
      "Implicit but\n",
      "Implicit because\n",
      "Implicit but\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    if pdtb2.loc[i, 'Relation'] == 'Explicit':\n",
    "        print(type(pdtb2.loc[i, 'Connective_SpanList']) == str)\n",
    "    if pdtb2.loc[i, 'Relation'] == 'Implicit':\n",
    "        print(pdtb2.loc[i, 'Relation'], pdtb2.loc[i, 'Conn1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linker(words, doc_lookup):\n",
    "    ## TODO: connective also has linker\n",
    "    \"\"\"\n",
    "    get look up for each word per doc level, if word does not below to any arg, it will append []\n",
    "    Args:\n",
    "            words(dict): {word_index_in_doc: [word_start_char, word_end_char]}\n",
    "            doc_lookup: from function @get_data_prototype\n",
    "    Returns: \n",
    "            ret: {word_index_in_doc: ['arg1_argID', 'arg2_argID', ...]}\n",
    "    \"\"\"\n",
    "    lookup_list = doc_lookup.keys()\n",
    "    ret = {}\n",
    "    \n",
    "    for w_idx, w_range in words.iteritems():\n",
    "        w_linkers = []\n",
    "        for r in doc_lookup.keys():\n",
    "            if _in_between_(w_range, r):\n",
    "                linker = doc_lookup[r][0]\n",
    "                w_linkers.append(linker)\n",
    "        ret[w_idx] = w_linkers\n",
    "    return ret\n",
    "\n",
    "def _in_between(inner_list, outer_list):\n",
    "    return (inner_list[0] >= outer_list[0]) & (inner_list[1] <= outer_list[1])\n",
    "    \n",
    "def get_batch(section, filenumber):\n",
    "    \"\"\"Returns list of index with all the file with the same format in that batch\"\"\"\n",
    "    return pdtb2.index(pdtb2['Section'] == section & pdtb2['filenumber'] == filenumber).tolist()\n",
    "        \n",
    "    \n",
    "def _get_span_list(span):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            span(str): \"34..96;97..101\"\n",
    "    Returns:\n",
    "            ret(list[list]):\n",
    "            if one arg has one segment: [[34,96]]\n",
    "            if one arg has two segment: example [[34, 96], [97, 101]]\n",
    "    \"\"\"\n",
    "    spans = span.split(';')\n",
    "    return [o.split('..') for o in spans]\n",
    "        \n",
    "\n",
    "def get_data_prototype(section, filenumber, relation_id, batch_idx):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            section(str)\n",
    "            filenumber(str)\n",
    "            relation_id(int): relation_id is unique accross every relation, this is start relation_id\n",
    "            batch_idx(list[int]): list of corresponding int from same file\n",
    "    Returns:\n",
    "            doc_data: [{\"Arg1\": {\"CharacterSpanList\": [[4564, 4610]], \n",
    "                \"RawText\": \"\", \n",
    "                \"TokenList\": []}, # example: [4612, 4616, 888, 32, 11]\n",
    "                \"DocID\": \"wsj_1000\", \n",
    "                \"ID\": 15025, \n",
    "                \"Sense\": [\"Contingency.Condition\"], \n",
    "                \"Type\": \"Explicit\"},\n",
    "                {}, ...]\n",
    "            doc_lookup: dictionary contains all the span for argument 1 or argument 2 or conn\n",
    "                used for function @get_linker\n",
    "                {[34, 45]: [\"arg1_1234\"]}\n",
    "                \n",
    "            \n",
    "    \"\"\"\n",
    "#     batch_idx = get_batch(section, filenumber)\n",
    "    doc_data = []\n",
    "    doc_lookup = {}\n",
    "    for idx in batch_idx:\n",
    "        arg1_span = pdtb2.loc[idx, 'Arg1_SpanList']\n",
    "        arg1_char_span_list = _get_span_list(arg1_span)\n",
    "        arg1_rawtext = pdtb2.loc[idx, 'Arg1_RawText']\n",
    "        arg1_token_list = []\n",
    "        \n",
    "        arg2_span = pdtb2.loc[idx, 'Arg2_SpanList']\n",
    "        arg2_char_span_list = _get_span_list(arg2_span)\n",
    "        arg2_rawtext = pdtb2.loc[idx, 'Arg2_RawText']\n",
    "        arg2_token_list = []\n",
    "        \n",
    "        conn_span = pdtb2.loc[idx, 'Connective_SpanList']\n",
    "        conn_char_span_list = _get_span_list(conn_span)\n",
    "        conn_rawtext = pdtb2.loc[idx, 'Connective_RawText']\n",
    "        conn_token_list = []\n",
    "        \n",
    "        relation_type = pdtb2.loc[idx, 'Relation']\n",
    "        relation_sense = [pdtb2.loc[idx, 'ConnHeadSemClass1']]\n",
    "        if type(pdtb2.loc[idx, 'ConnHeadSemClass2']) == str:\n",
    "            relation_sense.append(pdtb2.loc[idx, 'ConnHeadSemClass2'])\n",
    "        Doc_id = section + '/' + filenumber\n",
    "        ID = relation_id+idx\n",
    "        \n",
    "        doc_lookup[arg1_char_span_list] = 'arg1_' + ID\n",
    "        doc_lookup[arg2_char_span_list] = 'arg2_' + ID\n",
    "        doc_lookup[conn_char_span_list] = 'conn_' + ID\n",
    "        \n",
    "        arg1 = {\"CharacterSpanList\": arg1_char_span_list, \"RawText\": arg1_rawtext, \"TokenList\": arg1_token_list}\n",
    "        arg2 = {\"CharacterSpanList\": arg2_char_span_list, \"RawText\": arg2_rawtext, \"TokenList\": arg2_token_list}\n",
    "        conn = {\"CharacterSpanList\": conn_span, \"RawText\": conn_rawtext, \"TokenList\": conn_token_list}\n",
    "        relation_dict = {\"Arg1\": arg1, \"Arg2\": arg2, \"DocID\": Doc_id, \"ID\": ID, \n",
    "                      \"Sense\": relation_sense, \"Type\": relation_type}\n",
    "        doc_data.append(relation_dict)\n",
    "        \n",
    "    return doc_data, doc_lookup\n",
    "\n",
    "def _get_files(data_path=DATA_PATH):\n",
    "    \"\"\"Returns (list) [('00', 'wsj_0000'), ..., ('24', 'wsj_2400']\"\"\"\n",
    "    sections = os.listdir(data_path)\n",
    "    ret = {}\n",
    "    for sec in sections:\n",
    "        ret[sec] = os.listdir(data_path/sec)\n",
    "    return ret\n",
    "\n",
    "def read_file(section, filename, data_path=DATA_PATH):\n",
    "    \"\"\"Returns the content in that file\"\"\"\n",
    "    with open(data_path/section/filename) as f:\n",
    "        file = f.read()\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constituent_parsing(sent):\n",
    "    return nlp.parse(sentence).replace('ROOT', \" \").replace('\\n', '').replace(' ', '')\n",
    "\n",
    "def dependency_parse(sent):\n",
    "    s = tokenizor(sent)\n",
    "    tokens = [token.text for token in s]\n",
    "    tokens.insert(0,'ROOT')\n",
    "    dependency = []\n",
    "    for tag, start, end in nlp.dependency_parse(sent):\n",
    "        dependency.append([tag, f'{tokens[start]}-{start}', f'{tokens[end]}-{end}'])\n",
    "    return dependency\n",
    "\n",
    "def pdtb_to_json(folder, name):\n",
    "    p = Path(folder)\n",
    "    with open(p/name) as f:\n",
    "        file = f.read()\n",
    "        doc = tokenizor(file)\n",
    "\n",
    "        sentences = []\n",
    "        for idno, sent in enumerate(doc.sents):\n",
    "            sent = str(sent)\n",
    "            if idno == 0:\n",
    "                continue\n",
    "            dependencies = dependency_parse(sent)\n",
    "            parsetree = constituent_parsing(sent)\n",
    "\n",
    "            words = []\n",
    "            sent_index = file.index(sent)\n",
    "            for token in tokenizor(sent):\n",
    "                begin = sent_index+token.idx\n",
    "                token_dict = {'CharacterOffsetBegin':begin, \"CharacterOffsetEnd\": begin+len(token)}\n",
    "                words.append([token.text, token_dict])\n",
    "            sentences.append({\"dependencies\": dependencies, \"parsetree\": parsetree, \"words\": words})\n",
    "    return json.dumps({name: {\"sentences\": sentences}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_index(doc):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "            doc: after tokenized document\n",
    "    Returns: \n",
    "            words(dict): {word_index_in_doc: [word_start_char, word_end_char]}\n",
    "    \"\"\"\n",
    "    words = {}\n",
    "    for tok in doc:\n",
    "        words[tok.i] = [tok.idx, tok.idx+len(tok)]\n",
    "    return words\n",
    "\n",
    "def add_token_list(doc_data, linkers, token_list):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "            doc_data: return value from @function get_data_prototype\n",
    "            linker(list[str]): [\"arg1_1234\", \"conn_1235\"]\n",
    "            token_list: append at token_list\n",
    "    \"\"\"\n",
    "    for linker in linkers:\n",
    "        arg_id = linker.split('_')[1]\n",
    "        \n",
    "        for rel in doc_data:\n",
    "            if rel[\"ID\"] == int(arg_id):\n",
    "                rel[linker.split('_')[0]][\"TokenList\"].append(token_list)\n",
    "\n",
    "def data_generator_per_doc(section, filenumber, relation_id, batch_idx, rawtext):\n",
    "    \"\"\"generate json format data for one document\n",
    "    Args:\n",
    "            section(str): 0~24\n",
    "            filenumber(str): 0~99\n",
    "            relation_id(int): relation_id is unique accross every relation, this is start relation_id\n",
    "            batch_idx(list[int]): list of corresponding int from same file\n",
    "            rawtext(str): rawtext of that doc\n",
    "    Returns:\n",
    "            pdtb-json\n",
    "            pdtb-data\n",
    "    \"\"\"\n",
    "    # for data\n",
    "    doc_data, doc_lookup = get_data_prototype(section, filenumber, relation_id, batch_idx)\n",
    "    doc = tokenizor(rawtext)\n",
    "    words = get_word_index(doc)\n",
    "    linkers = get_linker(words, doc_lookup)\n",
    "    \n",
    "    # for parse\n",
    "    temp = None\n",
    "    sentences = []\n",
    "    # for each token in document\n",
    "    for tok in doc:\n",
    "        ## TODO: here need to change\n",
    "        if temp == None:\n",
    "            temp == True\n",
    "            continue\n",
    "        else:\n",
    "            if str(tok.sent) != temp:\n",
    "                # end of the previous round\n",
    "                sentences.append({\"dependencies\": dependencies, \"parsetree\": parsetree, \n",
    "                                  \"words\": words, \"Linkers\": linkers})\n",
    "                \n",
    "                # begin of the new round\n",
    "                words = []\n",
    "                temp = str(tok.sent)\n",
    "                dependencies = dependency_parse(temp)\n",
    "                parsetree = constituent_parsing(temp)\n",
    "            \n",
    "            ## 5 attribute of each word\n",
    "            char_offset_begin = tok.idx\n",
    "            char_offset_end = tok.idx + len(tok)\n",
    "            token_offset_in_doc = tok.i\n",
    "            sentence_offset = list(doc.sents).index(tok.sent)\n",
    "            token_offset_in_sent = list(tok.sent).index(tok)\n",
    "            \n",
    "            linker = linkers[token_offset_in_doc]\n",
    "            token_dict = {'CharacterOffsetBegin': char_offset_begin, \n",
    "                          \"CharacterOffsetEnd\": char_offset_end, \n",
    "                          \"Linkers\": linker}\n",
    "            \n",
    "            token_list = [char_offset_begin, char_offset_end, token_offset_in_doc, \n",
    "                          sentence_offset, token_offset_in_sent]\n",
    "            add_linker()\n",
    "            \n",
    "#     doc = tokenizor(rawtext)\n",
    "#     sentences = []\n",
    "#     for idno, sent in enumerate(doc.sents):\n",
    "#         sent = str(sent)\n",
    "#         if idno == 0:\n",
    "#             continue\n",
    "#         dependencies = dependency_parse(sent)\n",
    "#         parsetree = constituent_parsing(sent)\n",
    "\n",
    "#         words = []\n",
    "#         sent_index = file.index(sent)\n",
    "#         for token in tokenizor(sent):\n",
    "#             begin = sent_index+token.idx\n",
    "#             token_dict = {'CharacterOffsetBegin':begin, \"CharacterOffsetEnd\": begin+len(token)}\n",
    "#             words.append([token.text, token_dict])\n",
    "#         sentences.append({\"dependencies\": dependencies, \"parsetree\": parsetree, \"words\": words})\n",
    "\n",
    "\n",
    "def data_generator():\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "            docs_parse(dict{docid: parse})\n",
    "            docs_data(list[relation])\n",
    "    \"\"\"\n",
    "    doc_paths = _get_files()\n",
    "    relation_id = 0\n",
    "    docs_parse = {}\n",
    "    docs_data = []\n",
    "    for doc_path in doc_paths:\n",
    "        section = doc_path[0]\n",
    "        filename = doc_path[1]\n",
    "        rawtext = read_file(section, filename)\n",
    "        \n",
    "        section = str(int(section))\n",
    "        filenumber = filename[-2:]\n",
    "        batch_idx = get_batch(section, filename)\n",
    "        \n",
    "        doc_parse, doc_data = data_generator_per_doc(section, filenumber, relation_id, batch_idx, rawtext)\n",
    "        relation_id += len(batch_idx)\n",
    "        \n",
    "        docs_parse[filename] = doc_parse\n",
    "        docs_data.extend(doc_data)\n",
    "    return docs_parse, docs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
