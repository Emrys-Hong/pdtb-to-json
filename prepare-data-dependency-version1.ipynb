{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from pdtb2 import CorpusReader, Datum\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "#%% md\n",
    "learn more things from [github](https://github.com/cgpotts/pdtb2)\n",
    "#%%\n",
    "iterator = CorpusReader('../pdtb2.csv').iter_data(display_progress=False)\n",
    "#%%\n",
    "for _ in range(17): \n",
    "    next(iterator)\n",
    "#%%\n",
    "d = next(iterator)\n",
    "#%%\n",
    "d.arg1_attribution_pos()\n",
    "#%%\n",
    "len(CorpusReader('../pdtb2.csv'))\n",
    "#%%\n",
    "next(iterator)\n",
    "d = next(iterator)\n",
    "#%%\n",
    "d.Relation, d.Section, d.FileNumber, d.Connective_Trees\n",
    "#%%\n",
    "d.FullRawText\n",
    "#%%\n",
    "d.Arg2_Trees\n",
    "#%%\n",
    "[list(o) for o in d.Arg1_GornList]\n",
    "#%% md\n",
    "## prepare dataset\n",
    "#%%\n",
    "import pandas as pd\n",
    "import spacy,en_core_web_sm\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "#%%\n",
    "pdtb2 = pd.read_csv('../pdtb2.csv', low_memory=False)\n",
    "#%%\n",
    "pdtb2.ix[4000, 'Section']\n",
    "#%%\n",
    "pdtb2.head()\n",
    "#%% md\n",
    "### file number\n",
    "#%%\n",
    "print(\"file number is: \" + f\"{pdtb2.ix[4, 'Section']}_{pdtb2.ix[4, 'FileNumber']}\")\n",
    "#%% md\n",
    "### imports\n",
    "#%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy,en_core_web_sm\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import re\n",
    "from pathlib import Path\n",
    "import json\n",
    "tokenizor = en_core_web_sm.load()\n",
    "nlp = StanfordCoreNLP(r'/home/pengfei/documents/stanford-corenlp-full-2018-10-05/')\n",
    "sentence = \"On Tuesday afternoon , Kemper told Bear , Stearns & Co. , General Electric Co. 's Kidder , Peabody & Co. unit , Morgan Stanley and Oppenheimer & Co. that it'll no longer do business with them because of their commitment to index arbitrage , officials inside and outside these firms confirmed .\"\n",
    "#%% md\n",
    "### constituent\n",
    "#%%\n",
    "def constituent_parsing(sent):\n",
    "    return nlp.parse(sentence).replace('ROOT', \" \").replace('\\n', '').replace(' ', '')\n",
    "#%%\n",
    "# constituent_parsing(sentence)\n",
    "#%% md\n",
    "### dependency\n",
    "#%%\n",
    "def dependency_parse(sent):\n",
    "    s = tokenizor(sent)\n",
    "    tokens = [token.text for token in s]\n",
    "    tokens.insert(0,'ROOT')\n",
    "    dependency = []\n",
    "    for tag, start, end in nlp.dependency_parse(sent):\n",
    "        dependency.append([tag, f'{tokens[start]}-{start}', f'{tokens[end]}-{end}'])\n",
    "    return dependency\n",
    "#%%\n",
    "# dependency_parse(sentence)\n",
    "#%% md\n",
    "## preparing pdtb --testing on trial\n",
    "#%%\n",
    "def pdtb_to_json(folder, name):\n",
    "    p = Path(folder)\n",
    "    with open(p/name) as f:\n",
    "        file = f.read()\n",
    "        doc = tokenizor(file)\n",
    "\n",
    "        sentences = []\n",
    "        for idno, sent in enumerate(doc.sents):\n",
    "            sent = str(sent)\n",
    "            if idno == 0:\n",
    "                continue\n",
    "            dependencies = dependency_parse(sent)\n",
    "            parsetree = constituent_parsing(sent)\n",
    "\n",
    "            words = []\n",
    "            sent_index = file.index(sent)\n",
    "            for token in tokenizor(sent):\n",
    "                begin = sent_index+token.idx\n",
    "                token_dict = {'CharacterOffsetBegin':begin, \"CharacterOffsetEnd\": begin+len(token)}\n",
    "                words.append([token.text, token_dict])\n",
    "            sentences.append({\"dependencies\": dependencies, \"parsetree\": parsetree, \"words\": words})\n",
    "    return json.dumps({name: {\"sentences\": sentences}})\n",
    "#%%\n",
    "jsonFile = pdtb_to_json('/home/pengfei/data/trial/raw_train/', 'wsj_1000')\n",
    "#%%\n",
    "with open('pdtb-parses.json', 'w') as f:\n",
    "    json.dump(jsonFile, f)\n",
    "#%%\n",
    "with open('a.txt', 'w') as f:\n",
    "    f.write('a')\n",
    "#%%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
